{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the best policy for the random walk problem using Q table n-step Sarse.\n",
    "\n",
    "The random walk problem is defined at the book (Chapter 6.2, page 133, example 6.2). We make the chain a bit bigger with a path with 9 states with a terminating state on each side. The rewards will be 1 at the right most terminating state, -1 at the left most and 0 elsewhere.\n",
    "\n",
    "For the solution, we follow the pseudo code from chapter 7.2, page 157."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def arg_max_with_tol(arr, tol=0.0001):\n",
    "    \"\"\"\n",
    "    Finds the index of the max value in an array. Allows precision error to be ignored. Breaks even randomely.\n",
    "    \"\"\"\n",
    "    m = max(arr)\n",
    "    l = []\n",
    "    \n",
    "    for i, val in enumerate(arr):\n",
    "        if abs(val - m) <= tol:\n",
    "            l.append(i)\n",
    "    tmp = random.randint(0, len(l) - 1)\n",
    "    return l[tmp]\n",
    "\n",
    "def choose_action_idx(actions, state_q_table, epsilon=0.1):\n",
    "    \"\"\"Chosses the action idx according to greedy epsilon policy. \"\"\"\n",
    "    tmp = random.random()\n",
    "    if tmp < epsilon:\n",
    "        a = random.randint(0, len(actions) - 1)\n",
    "    else:\n",
    "        a = arg_max_with_tol(state_q_table)\n",
    "    return a\n",
    "\n",
    "def get_greedy_policy(s, q_table, actions):\n",
    "    \"\"\"Returns the sequence of states visited starting from s. Uses greedy policy breaking even randomely. \"\"\"\n",
    "    states = [s]\n",
    "    while s not in Ts:\n",
    "        a = choose_action_idx(actions, q_table[s], epsilon=0)\n",
    "        s = s + actions[a]\n",
    "        states.append(s)\n",
    "    \n",
    "    return states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing the world with $\\alpha = 0.3, n = 3, \\gamma=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Ts = {0, 10}\n",
    "n = 3\n",
    "alpha = 0.3\n",
    "gamma = 1\n",
    "actions = [-1, 1]\n",
    "Rs = ([0] * 11)\n",
    "Rs[10] = 1  # The last state is the only rewarded state\n",
    "Rs[0] = -1\n",
    "q_table = defaultdict(lambda: ([0] * len(actions)), {})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's run a single episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "S, A = [], []\n",
    "S0 = 5\n",
    "S.append(S0)\n",
    "A0 = choose_action_idx(actions, q_table[S0])\n",
    "A.append(A0)\n",
    "T = sys.maxint\n",
    "for t in xrange(sys.maxint):    # What will happen if you'll use range instead of xrange\n",
    "    if t < T:\n",
    "        # Taking action At\n",
    "        next_s = S[t] + actions[A[t]]\n",
    "        S.append(next_s)\n",
    "        next_r = Rs[next_s]\n",
    "    \n",
    "        # Checking if it's in terminate states\n",
    "        if next_s in Ts:\n",
    "            T = t + 1\n",
    "        else:\n",
    "            next_a = choose_action_idx(actions, q_table[next_s])\n",
    "            A.append(next_a)\n",
    "    \n",
    "    tau = t - n + 1\n",
    "    if T < sys.maxint:  # This is not in the psuedo code - but it should be\n",
    "        tau = max(tau, 0)\n",
    "        \n",
    "    if tau >= 0:\n",
    "        G = sum([gamma ** (i - tau - 1) * Rs[S[i]] for i in range(tau + 1, min(tau + n, T) + 1)])\n",
    "        if tau + n < T:\n",
    "            G += gamma ** n * q_table[S[tau + n]][A[tau + n]] \n",
    "\n",
    "        old_q = q_table[S[tau]][A[tau]]\n",
    "        q_table[S[tau]][A[tau]] = old_q + alpha * (G - old_q)        \n",
    "    \n",
    "    if tau == T - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the greedy policy for different states. \n",
    "\n",
    "### Question:\n",
    "Which states can we be sure that they are correct at this point? Why is that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 8, 9, 8, 9, 10]\n",
      "[8, 9, 10]\n",
      "[7, 8, 9, 10]\n",
      "[5, 6, 7, 6, 5, 6, 7, 8, 9, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "print get_greedy_policy(9, q_table, actions)\n",
    "print get_greedy_policy(8, q_table, actions)\n",
    "print get_greedy_policy(7, q_table, actions)\n",
    "print get_greedy_policy(5, q_table, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "How can we compute convergance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expected_num_steps(s, q_table, actions, size=10000):\n",
    "    \"\"\"Computes the expected number of steps until termination starting from state s. \"\"\"\n",
    "    num_steps = 0\n",
    "    for i in range(size):\n",
    "        num_steps += len(get_greedy_policy(s, q_table, actions)) - 1\n",
    "    \n",
    "    return num_steps / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.2716"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_num_steps(5, q_table, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running until convergance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_num_steps = float('inf')\n",
    "converged = False\n",
    "while not converged:\n",
    "    S, A = [], []\n",
    "    S0 = 5\n",
    "    S.append(S0)\n",
    "    A0 = choose_action_idx(actions, q_table[S0])\n",
    "    A.append(A0)\n",
    "    T = sys.maxint\n",
    "    for t in xrange(sys.maxint):    # What will happen if you'll use range instead of xrange\n",
    "        if t < T:\n",
    "            # Taking action At\n",
    "            next_s = S[t] + actions[A[t]]\n",
    "            S.append(next_s)\n",
    "            next_r = Rs[next_s]\n",
    "\n",
    "            # Checking if it's in terminate states\n",
    "            if next_s in Ts:\n",
    "                T = t + 1\n",
    "            else:\n",
    "                next_a = choose_action_idx(actions, q_table[next_s])\n",
    "                A.append(next_a)\n",
    "\n",
    "        tau = t - n + 1\n",
    "        if T < sys.maxint:  # This is not in the psuedo code - but it should be\n",
    "            tau = max(tau, 0)\n",
    "            \n",
    "        if tau >= 0:\n",
    "            G = sum([gamma ** (i - tau - 1) * Rs[S[i]] for i in range(tau + 1, min(tau + n, T) + 1)])\n",
    "            if tau + n < T:\n",
    "                G += gamma ** n * q_table[S[tau + n]][A[tau + n]] \n",
    "\n",
    "            old_q = q_table[S[tau]][A[tau]]\n",
    "            q_table[S[tau]][A[tau]] = old_q + alpha * (G - old_q)        \n",
    "\n",
    "        if tau == T - 1:\n",
    "            break\n",
    "            \n",
    "    # Checking for convergance.            \n",
    "    tmp = expected_num_steps(5, q_table, actions)\n",
    "    if np.abs(exp_num_steps - tmp) < 0.0001:\n",
    "        converged = True\n",
    "    \n",
    "    exp_num_steps = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 8, 9, 10]\n",
      "5.0\n"
     ]
    }
   ],
   "source": [
    "print get_greedy_policy(5, q_table, actions)\n",
    "print expected_num_steps(5, q_table, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "Are we guarenteed to learn anything for state 2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 2, 1, 2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[2, 3, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[2, 3, 4, 3, 2, 3, 4, 3, 2, 1, 0]\n",
      "[2, 3, 2, 3, 2, 3, 2, 1, 2, 3, 2, 1, 2, 3, 2, 1, 2, 3, 2, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "print get_greedy_policy(2, q_table, actions)\n",
    "print get_greedy_policy(2, q_table, actions)\n",
    "print get_greedy_policy(2, q_table, actions)\n",
    "print get_greedy_policy(2, q_table, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unless we increase the epsilon (which we don't really want to do -- it's too high already) we are not guarenteed to visit node 2 (or 3 or 4). That means that the q_table value is [0, 0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fix that by sampling an initial state and checking for convergance starting at state 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_num_steps = float('inf')\n",
    "converged = False\n",
    "while not converged:\n",
    "    S, A = [], []\n",
    "    S0 = random.randint(2, 9)   # Sampling the starting point\n",
    "    S.append(S0)\n",
    "    A0 = choose_action_idx(actions, q_table[S0])\n",
    "    A.append(A0)\n",
    "    T = sys.maxint\n",
    "    for t in xrange(sys.maxint):    # What will happen if you'll use range instead of xrange\n",
    "        if t < T:\n",
    "            # Taking action At\n",
    "            next_s = S[t] + actions[A[t]]\n",
    "            S.append(next_s)\n",
    "            next_r = Rs[next_s]\n",
    "\n",
    "            # Checking if it's in terminate states\n",
    "            if next_s in Ts:\n",
    "                T = t + 1\n",
    "            else:\n",
    "                next_a = choose_action_idx(actions, q_table[next_s])\n",
    "                A.append(next_a)\n",
    "\n",
    "        tau = t - n + 1\n",
    "        if T < sys.maxint:  # This is not in the psuedo code - but it should be\n",
    "            tau = max(tau, 0)\n",
    "            \n",
    "        if tau >= 0:\n",
    "            G = sum([gamma ** (i - tau - 1) * Rs[S[i]] for i in range(tau + 1, min(tau + n, T) + 1)])\n",
    "            if tau + n < T:\n",
    "                G += gamma ** n * q_table[S[tau + n]][A[tau + n]] \n",
    "\n",
    "            old_q = q_table[S[tau]][A[tau]]\n",
    "            q_table[S[tau]][A[tau]] = old_q + alpha * (G - old_q)        \n",
    "\n",
    "        if tau == T - 1:\n",
    "            break\n",
    "            \n",
    "    # Checking for convergance.\n",
    "    tmp = expected_num_steps(2, q_table, actions)\n",
    "    if np.abs(exp_num_steps - tmp) < 0.0001:\n",
    "        converged = True\n",
    "    \n",
    "    exp_num_steps = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "8.0\n"
     ]
    }
   ],
   "source": [
    "print get_greedy_policy(2, q_table, actions)\n",
    "print expected_num_steps(2, q_table, actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "Would this solution work for state 1 as well?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
